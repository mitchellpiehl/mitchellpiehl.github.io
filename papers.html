<!DOCTYPE html>
<html lang="en">
    <head>
        <title>AI History - Papers</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" href="portfolio.css" />
        <link rel="stylesheet" href="styles.css" />
    </head>
    <body>
        <div id="header">
            <div class="hcol1">
                <img src="images/StThomasLogo.png" class="img"/>
            </div>
            <div class="hcol2"><h1>The History of Artificial Intelligence</h1></div>
        </div>
        <div id="header2">
            <div>
                <nav id="main-nav">
                    <ul id="nav">
                        <li><a href="index.html">Home</a></li>
                        <li><a href="timeline.html">Timeline</a></li>
                        <li><a href="people.html">People</a></li>
                        <li><a href="papers.html" class="current">Papers</a></li>
                        <li><a href="resources.html">Resources</a></li>
                        <li><a href="links.html">Interactive Links</a></li>
                    </ul>
                </nav>
                <button id="mobile-menu-toggle" onclick="toggleMobileMenu()">
                    <span></span>
                    <span></span>
                    <span></span>
                </button>
            </div>
        </div>
    <div class="paper-container">
        <div class="paper">
            <h2>Language Models are unsupervised multitask learners - 2019</h2>
            <p>
                Paper from OpenAI that releases GPT-2. A transformer power large language model that demonstrates the abilities of langugage models to perform large a large variety of tasks. 
            </p>
            <p>
                Click  
                <a href="https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf">here</a>
                 to view the paper.
            </p>
        </div>
        <div class="paper">
            <h2>Attention is All You Need - 2017</h2>
            <p>
                Paper written by Google that releases the transformer. The transformer is a deep learning architecture that uses multi-headed attention 
                and processes inputs in parellel to get more accurate outputs. 
                Completely revolutionized the natural language processing world by its implications in large language models such as ChatGPT.
            </p>
            <p>
                Click  
                <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">here</a>
                 to view the paper.
            </p>
        </div>
        <div class="paper">
            <h2>Efficient Estimation of Word Representations in Vector Space - 2013</h2>
            <p>
                Introduces word vectors (or Word2Vec); a way to store words in a vector space so that words with similar meanings contain similar values to the words around it.
                This allows for much more efficient and accurate processing of words in natural language processing tasks. 
            </p>
            <p>
                Click  
                <a href="https://www.khoury.northeastern.edu/home/vip/teach/DMcourse/4_TF_supervised/notes_slides/1301.3781.pdf">here</a>
                 to view the paper.
            </p>
        </div>
        <div class="paper">
            <h2>ImageNet Classification with Deep Convolutional Neural Networks - 2012</h2>
            <p>
                Uses the convolutional neural network (CNN) to classify ImageNet images with significantly greater accuracy than ever before. 
                The paper completely revitalized neural networks, showing their large potential and inspiring researchers to dive deeper into neural networks than ever before. 
            </p>
            <p>
                Click  
                <a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">here</a>
                 to view the paper.
            </p>
        </div>
        <div class="paper">
            <h2>ImageNet: A large-scale hierarchical image database - 2009</h2>
            <p>
                Introduces ImageNet; a image dataset containing 3.2 million labled images, allowing researchers to train more advanced models on a dataset magnitudes larger than before.
                This dataset revolutionized deep learning and big data, creating the birth of the deep learning revolution. 
            </p>
            <p>
                Click  
                <a href="https://ieeexplore.ieee.org/abstract/document/5206848">here</a>
                 to view the paper.
            </p>
        </div>
        <div class="paper">
            <h2>Learning representations by back-propagating errors - 1986</h2>
            <p>
                Introduces a new learning procedure in neural networks called back-propagtion. 
                Back-propagation allows neural networks to adjust the weights within hidden layers to effectively "learn" and provide more accurate outputs. 
            </p>
            <p>
                Click  
                <a href="https://www.nature.com/articles/323533a0">here</a>
                 to view the paper.
            </p>
        </div>
    </div>
    <script>
        function toggleMobileMenu() {
            var nav = document.getElementById("main-nav");
            nav.classList.toggle("show-mobile");
        }
    </script>
</body>
</html>
